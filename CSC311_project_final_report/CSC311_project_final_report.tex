\documentclass{article}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{hyperref}
\usepackage{float}
\usepackage{subcaption} % for subfigures

\title{CSC311 Project Final Report}
\author{Group Members: \\ Zixuan Zeng 1008533419}

\begin{document}

\maketitle

\section*{Part A}

\section*{Q1}

\subsection*{1.(a)}

\begin{figure}[H]
    \centering
    \begin{subfigure}{0.45\textwidth}
        \includegraphics[width=\textwidth]{knn_impute_by_user_result.png}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.35\textwidth}
        \includegraphics[width=\textwidth]{knn_impute_by_user_plot.png}
    \end{subfigure}
    \caption{Accuracy vs k for KNN Impute by User}
\end{figure}

\subsection*{1.(b)}

\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{knn_impute_by_user_test.png}
    \caption{Test accuracy with k*}
\end{figure}

\subsection*{1.(c)}

The underlying assumption is that if question A is answered similarly by many students as question B, A’s predicted response from specific students matches that of question B.

\begin{figure}[H]
    \centering
    \begin{subfigure}{0.45\textwidth}
        \includegraphics[width=\textwidth]{knn_impute_by_item_result.png}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.35\textwidth}
        \includegraphics[width=\textwidth]{knn_impute_by_item_plot.png}
    \end{subfigure}
    \caption{Accuracy vs k for KNN Impute by Item}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{knn_impute_by_item_test.png}
    \caption{Test Accuracy with k*}
\end{figure}

\subsection*{1.(d)}
The test accuracy for the user-based method (0.6842) is higher than that for the item-based method (0.6684). Therefore, the user-based collaborative filtering method performs better than the item-based collaborative filtering method in this case.

\subsection*{1.(e)}
\begin{itemize}
    \item Computationally expensive. KNN practically has no training process. With large datasets, as the number of students/questions grow, the time required to compute the distances and to identify the nearest neighbors at test time grows significantly.
    \item Curse of Dimensionality. When the sparse\_matrix has too many missing values, it’s hard to find good nearest neighbors, since most points will be about the same distances. This affects the prediction accuracy.
\end{itemize}


\newpage
\section*{Q2}

\subsection*{2.(a)}

Given the probability that student $i$ correctly answers question $j$ as:

\[
p(c_{ij} = 1 | \theta_i, \beta_j) = \frac{\exp(\theta_i - \beta_j)}{1 + \exp(\theta_i - \beta_j)}
\]

The log-likelihood for all students and questions, given the sparse matrix $C$, is:

\[
\log p(C | \theta, \beta) = \sum_{(i,j) \in \text{observed}} \left[ c_{ij} (\theta_i - \beta_j) - \log (1 + \exp(\theta_i - \beta_j)) \right]
\]

The derivative of the log-likelihood with respect to the ability parameter $\theta_i$ is:

\[
\frac{\partial \log p(C | \theta, \beta)}{\partial \theta_i} = \sum_{j \in \text{observed}} \left[ c_{ij} - \sigma(\theta_i - \beta_j) \right]
\]

The derivative of the log-likelihood with respect to the difficulty parameter $\beta_j$ is:

\[
\frac{\partial \log p(C | \theta, \beta)}{\partial \beta_j} = \sum_{i \in \text{observed}} \left[ - c_{ij} + \sigma(\theta_i - \beta_j) \right]
\]


\newpage
\section*{Part B}

\end{document}
